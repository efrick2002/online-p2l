proj_name: Llama-3.1-8B-Instruct-full-train
learning_rate: 4.0e-6
adam_epsilon: 1.0e-8
batch_size: 4
max_length: 8192
num_train_epochs: 1
train_data_path: full-p2l-data
val_data_path: p2el/canonical_bt_val_data_11092024
output_dir: 'training_outputs'
pretrain_model_name: meta-llama/Llama-3.1-8B-Instruct
gradient_accumulation_steps: 16 # drop to 32 since 8 gpus
chat_template: "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"
model_type: "llama"
head_type: "bt"
loss_type: "bt_tie"
weighted_loss: false
deepspeed_config_path: deepspeed/zero1.json
init_type: reset_params
load_train_data_from_disk: true
pad_token_if_none: <|finetune_right_pad_id|>
cls_token_if_none: <|reserved_special_token_3|>